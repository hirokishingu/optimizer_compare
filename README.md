# zrAI

> ゼロから作るDeepLearning学習用


## ch6 学習テクニック

- std = 0.01　の時に学習が行われない
- Batch Norm　は適切な初期値を設定することで角層のactivationの分布が広がるが、それを”強制的”に調整する.
## batch norm 利点
- 学習係数が大きいから学習が早い
- 初期値に依存しない
- か学習を抑制する


## ch7 畳み込みニューラルネットワーク

- paddingは出力サイズの調整のために行う.






Hiroki Shingu
